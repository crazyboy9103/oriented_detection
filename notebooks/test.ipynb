{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pretrained weights are not used for training, freeze_bn is ignored.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 483\u001b[0m\n\u001b[1;32m    480\u001b[0m     fast_rcnn_norm_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBatchNorm2d\n\u001b[1;32m    481\u001b[0m     backbone_norm_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBatchNorm2d\n\u001b[0;32m--> 483\u001b[0m trainable_backbone_layers \u001b[39m=\u001b[39m _validate_trainable_layers(is_backbone_trained, trainable_backbone_layers, max_value\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, default_value\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m    485\u001b[0m backbone \u001b[39m=\u001b[39m resnet50(weights\u001b[39m=\u001b[39mweights_backbone, progress\u001b[39m=\u001b[39mprogress)\n\u001b[1;32m    486\u001b[0m backbone \u001b[39m=\u001b[39m _resnet_fpn_extractor(backbone, trainable_backbone_layers, [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m], norm_layer\u001b[39m=\u001b[39mbackbone_norm_layer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/backbone_utils.py:170\u001b[0m, in \u001b[0;36m_validate_trainable_layers\u001b[0;34m(is_trained, trainable_backbone_layers, max_value, default_value)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m trainable_backbone_layers \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     trainable_backbone_layers \u001b[39m=\u001b[39m default_value\n\u001b[0;32m--> 170\u001b[0m \u001b[39mif\u001b[39;00m trainable_backbone_layers \u001b[39m<\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39mor\u001b[39;00m trainable_backbone_layers \u001b[39m>\u001b[39m max_value:\n\u001b[1;32m    171\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    172\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrainable backbone layers should be in the range [0,\u001b[39m\u001b[39m{\u001b[39;00mmax_value\u001b[39m}\u001b[39;00m\u001b[39m], got \u001b[39m\u001b[39m{\u001b[39;00mtrainable_backbone_layers\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m     )\n\u001b[1;32m    174\u001b[0m \u001b[39mreturn\u001b[39;00m trainable_backbone_layers\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Tuple, Dict, Any, Literal\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "from torchvision.models.detection.faster_rcnn import TwoMLPHead, FastRCNNConvFCHead\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "# Backbones\n",
    "from torchvision.models.resnet import resnet18, resnet50, resnet101, resnet152\n",
    "from torchvision.models.detection.backbone_utils import _mobilenet_extractor, _resnet_fpn_extractor, _validate_trainable_layers\n",
    "# Weights\n",
    "from torchvision.models.detection.faster_rcnn import (\n",
    "    FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    "    FasterRCNN_MobileNet_V3_Large_FPN_Weights,\n",
    "    FasterRCNN_ResNet50_FPN_Weights, \n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    ")\n",
    "from torchvision.models.resnet import (\n",
    "    ResNet18_Weights, \n",
    "    ResNet50_Weights, \n",
    "    ResNet101_Weights, \n",
    "    ResNet152_Weights\n",
    ")\n",
    "\n",
    "from models.detection.roi_heads import RoIHeads as FasterRCNNRoIHeads\n",
    "from models.detection.oriented_roi_heads import RoIHeads as OrientedRCNNRoIHeads\n",
    "from models.detection.rpn import RPNHead, OrientedRPNHead, RegionProposalNetwork, OrientedRegionProposalNetwork\n",
    "from models.detection.transform import GeneralizedRCNNTransform\n",
    "from ops.poolers import MultiScaleRotatedRoIAlign\n",
    "\n",
    "def _default_anchor_generator():\n",
    "    sizes = ((4, 8, 16, 32, 64,),) * 5 \n",
    "    ratios = ((0.5, 1.0, 2.0),) * len(sizes)\n",
    "    return AnchorGenerator(sizes=sizes, aspect_ratios=ratios)\n",
    "\n",
    "def _check_for_degenerate_boxes(targets):\n",
    "    for target_idx, target in enumerate(targets):\n",
    "        boxes = target[\"bboxes\"]\n",
    "        degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
    "        if degenerate_boxes.any():\n",
    "            # print the first degenerate box\n",
    "            bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n",
    "            degen_bb: List[float] = boxes[bb_idx].tolist()\n",
    "            torch._assert(\n",
    "                False,\n",
    "                \"All bounding boxes should have positive height and width.\"\n",
    "                f\" Found invalid box {degen_bb} for target at index {target_idx}.\",\n",
    "            )\n",
    "        \n",
    "        oboxes = target[\"oboxes\"]\n",
    "        degenerate_oboxes = oboxes[:, 2:4] <= 0\n",
    "        if degenerate_oboxes.any():\n",
    "            # print the first degenerate box\n",
    "            bb_idx = torch.where(degenerate_oboxes.any(dim=1))[0][0]\n",
    "            degen_bb: List[float] = oboxes[bb_idx].tolist()\n",
    "            torch._assert(\n",
    "                False,\n",
    "                \"All bounding boxes should have positive height and width.\"\n",
    "                f\" Found invalid box {degen_bb} for target at index {target_idx}.\",\n",
    "            )\n",
    "\n",
    "class RotatedFastRCNNPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + bounding box + angle regression layers \n",
    "    for Fast R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
    "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
    "        self.obbox_pred = nn.Linear(in_channels, num_classes * 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            torch._assert(\n",
    "                list(x.shape[2:]) == [1, 1],\n",
    "                f\"x has the wrong shape, expecting the last two dimensions to be [1,1] instead of {list(x.shape[2:])}\",\n",
    "            )\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        bbox_deltas = self.bbox_pred(x)\n",
    "        obbox_deltas = self.obbox_pred(x)\n",
    "        return scores, bbox_deltas, obbox_deltas\n",
    "\n",
    "class OrientedRCNNPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard classification + Oriented bounding box regression layers \n",
    "    for Oriented R-CNN.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        num_classes (int): number of output classes (including background)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
    "        self.obbox_pred = nn.Linear(in_channels, num_classes * 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            torch._assert(\n",
    "                list(x.shape[2:]) == [1, 1],\n",
    "                f\"x has the wrong shape, expecting the last two dimensions to be [1,1] instead of {list(x.shape[2:])}\",\n",
    "            )\n",
    "        x = x.flatten(start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        obbox_deltas = self.obbox_pred(x)\n",
    "        return scores, obbox_deltas\n",
    "    \n",
    "class GeneralizedRCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        backbone: nn.Module,                \n",
    "        transform: nn.Module,\n",
    "        rpn: nn.Module,\n",
    "        roi_heads: nn.Module,\n",
    "    ) -> None:\n",
    "        super(GeneralizedRCNN, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.transform = transform\n",
    "        self.rpn = rpn\n",
    "        self.roi_heads = roi_heads\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                During training, it returns a dict[Tensor] which contains the losses.\n",
    "                During testing, it returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "\n",
    "        \"\"\"\n",
    "        original_image_sizes: List[Tuple[int, int]] = []\n",
    "        for img in images:\n",
    "            val = img.shape[-2:]\n",
    "            torch._assert(\n",
    "                len(val) == 2,\n",
    "                f\"expecting the last two dimensions of the Tensor to be H and W instead got {img.shape[-2:]}\",\n",
    "            )\n",
    "            original_image_sizes.append((val[0], val[1]))\n",
    "\n",
    "        images, targets = self.transform(images, targets)\n",
    "\n",
    "        if targets is not None:\n",
    "            _check_for_degenerate_boxes(targets)\n",
    "\n",
    "        features = self.backbone(images.tensors)\n",
    "        if isinstance(features, torch.Tensor):\n",
    "            features = OrderedDict([(\"0\", features)])\n",
    "            \n",
    "        proposals, proposal_losses = self.rpn(images, features, targets)\n",
    "        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
    "        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n",
    "\n",
    "        losses = {}\n",
    "        losses.update(detector_losses)\n",
    "        losses.update(proposal_losses)\n",
    "\n",
    "        if torch.jit.is_scripting():\n",
    "            return losses, detections\n",
    "        else:\n",
    "            if self.training:\n",
    "                return losses\n",
    "            \n",
    "            if targets:\n",
    "                return losses, detections\n",
    "            \n",
    "            return detections\n",
    "        \n",
    "class RotatedFasterRCNN(GeneralizedRCNN):\n",
    "    def __init__(\n",
    "        self, \n",
    "        backbone: nn.Module,                \n",
    "        num_classes: int = 16,\n",
    "        # transform parameters\n",
    "        min_size=800,\n",
    "        max_size=1333,\n",
    "        image_mean=None,\n",
    "        image_std=None,\n",
    "        # RPN parameters\n",
    "        rpn_anchor_generator: nn.Module = _default_anchor_generator(),\n",
    "        rpn_head=None,\n",
    "        rpn_pre_nms_top_n_train=2000,\n",
    "        rpn_pre_nms_top_n_test=1000,\n",
    "        rpn_post_nms_top_n_train=2000,\n",
    "        rpn_post_nms_top_n_test=1000,\n",
    "        rpn_nms_thresh=0.7,\n",
    "        rpn_fg_iou_thresh=0.7,\n",
    "        rpn_bg_iou_thresh=0.3,\n",
    "        rpn_batch_size_per_image=256,\n",
    "        rpn_positive_fraction=0.5,\n",
    "        rpn_score_thresh=0.0,\n",
    "        # Box parameters\n",
    "        box_roi_pool: nn.Module = MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=7, sampling_ratio=0),\n",
    "        box_head: nn.Module = None,\n",
    "        box_predictor: nn.Module = None,\n",
    "        box_score_thresh=0.05,\n",
    "        box_nms_thresh=0.5,\n",
    "        box_detections_per_img=100,\n",
    "        box_fg_iou_thresh=0.5,\n",
    "        box_bg_iou_thresh=0.5,\n",
    "        box_batch_size_per_image=512,\n",
    "        box_positive_fraction=0.25,\n",
    "        bbox_reg_weights=None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        out_channels = backbone.out_channels\n",
    "        \n",
    "        if rpn_head is None:\n",
    "            rpn_head = RPNHead(out_channels, rpn_anchor_generator.num_anchors_per_location()[0])\n",
    "        \n",
    "        if box_head is None:\n",
    "            resolution = box_roi_pool.output_size[0]\n",
    "            box_head = TwoMLPHead(in_channels=out_channels * resolution ** 2, representation_size=1024)\n",
    "        \n",
    "        if box_predictor is None:\n",
    "            box_predictor = RotatedFastRCNNPredictor(in_channels=1024, num_classes=num_classes)\n",
    "            \n",
    "        if image_mean is None:\n",
    "            image_mean = [0.485, 0.456, 0.406]\n",
    "        if image_std is None:\n",
    "            image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std, **kwargs)\n",
    "        \n",
    "        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n",
    "        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n",
    "        rpn = RegionProposalNetwork(\n",
    "            rpn_anchor_generator,\n",
    "            rpn_head,\n",
    "            rpn_fg_iou_thresh,\n",
    "            rpn_bg_iou_thresh,\n",
    "            rpn_batch_size_per_image,\n",
    "            rpn_positive_fraction,\n",
    "            rpn_pre_nms_top_n,\n",
    "            rpn_post_nms_top_n,\n",
    "            rpn_nms_thresh,\n",
    "            score_thresh=rpn_score_thresh,\n",
    "        )\n",
    "        \n",
    "        roi_heads = FasterRCNNRoIHeads(\n",
    "            # Box\n",
    "            box_roi_pool,\n",
    "            box_head,\n",
    "            box_predictor,\n",
    "            box_fg_iou_thresh,\n",
    "            box_bg_iou_thresh,\n",
    "            box_batch_size_per_image,\n",
    "            box_positive_fraction,\n",
    "            bbox_reg_weights,\n",
    "            box_score_thresh,\n",
    "            box_nms_thresh,\n",
    "            box_detections_per_img,\n",
    "            nms_thresh_rotated = 0.1\n",
    "        )\n",
    "                \n",
    "        super(RotatedFasterRCNN, self).__init__(backbone, transform, rpn, roi_heads)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                During training, it returns a dict[Tensor] which contains the losses.\n",
    "                During testing, it returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            if targets is None:\n",
    "                torch._assert(False, \"targets should not be none when in training mode\")\n",
    "            else:\n",
    "                for target in targets:\n",
    "                    boxes = target[\"bboxes\"]\n",
    "                    if isinstance(boxes, torch.Tensor):\n",
    "                        torch._assert(\n",
    "                            len(boxes.shape) == 2 and boxes.shape[-1] == 4,\n",
    "                            f\"Expected target boxes to be a tensor of shape [N, 4], got {boxes.shape}.\",\n",
    "                        )\n",
    "                    else:\n",
    "                        torch._assert(False, f\"Expected target boxes to be of type Tensor, got {type(boxes)}.\")\n",
    "                        \n",
    "                    oboxes = target[\"oboxes\"]\n",
    "                    if isinstance(oboxes, torch.Tensor):\n",
    "                        torch._assert(\n",
    "                            len(oboxes.shape) == 2 and oboxes.shape[-1] == 5,\n",
    "                            f\"Expected target boxes to be a tensor of shape [N, 5], got {oboxes.shape}.\",\n",
    "                        )\n",
    "                    else:\n",
    "                        torch._assert(False, f\"Expected target boxes to be of type Tensor, got {type(oboxes)}.\")\n",
    "\n",
    "        return super(RotatedFasterRCNN, self).forward(images, targets)\n",
    "\n",
    " \n",
    "class OrientedRCNN(GeneralizedRCNN):\n",
    "    def __init__(\n",
    "        self, \n",
    "        backbone: nn.Module,                \n",
    "        num_classes: int = 16,\n",
    "        # transform parameters\n",
    "        min_size=800,\n",
    "        max_size=1333,\n",
    "        image_mean=None,\n",
    "        image_std=None,\n",
    "        # RPN parameters\n",
    "        rpn_anchor_generator: nn.Module = _default_anchor_generator(),\n",
    "        rpn_head=None,\n",
    "        rpn_pre_nms_top_n_train=2000,\n",
    "        rpn_pre_nms_top_n_test=1000,\n",
    "        rpn_post_nms_top_n_train=2000,\n",
    "        rpn_post_nms_top_n_test=1000,\n",
    "        rpn_nms_thresh=0.7,\n",
    "        rpn_fg_iou_thresh=0.7,\n",
    "        rpn_bg_iou_thresh=0.3,\n",
    "        rpn_batch_size_per_image=256,\n",
    "        rpn_positive_fraction=0.5,\n",
    "        rpn_score_thresh=0.0,\n",
    "        # Box parameters\n",
    "        box_roi_pool: nn.Module = MultiScaleRotatedRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=7, sampling_ratio=0),\n",
    "        box_head: nn.Module = None,\n",
    "        box_predictor: nn.Module = None,\n",
    "        box_score_thresh=0.05,\n",
    "        box_nms_thresh=0.5,\n",
    "        box_detections_per_img=100,\n",
    "        box_fg_iou_thresh=0.5,\n",
    "        box_bg_iou_thresh=0.5,\n",
    "        box_batch_size_per_image=512,\n",
    "        box_positive_fraction=0.25,\n",
    "        bbox_reg_weights=None,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        out_channels = backbone.out_channels\n",
    "        \n",
    "        if rpn_head is None:\n",
    "            rpn_head = OrientedRPNHead(out_channels, rpn_anchor_generator.num_anchors_per_location()[0])\n",
    "        \n",
    "        if box_head is None:\n",
    "            resolution = box_roi_pool.output_size[0]\n",
    "            box_head = TwoMLPHead(in_channels=out_channels * resolution ** 2, representation_size=1024)\n",
    "        \n",
    "        if box_predictor is None:\n",
    "            box_predictor = OrientedRCNNPredictor(in_channels=1024, num_classes=num_classes)\n",
    "            \n",
    "        if image_mean is None:\n",
    "            image_mean = [0.485, 0.456, 0.406]\n",
    "        if image_std is None:\n",
    "            image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std, **kwargs)\n",
    "        \n",
    "        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n",
    "        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n",
    "        rpn = OrientedRegionProposalNetwork(\n",
    "            rpn_anchor_generator,\n",
    "            rpn_head,\n",
    "            rpn_fg_iou_thresh,\n",
    "            rpn_bg_iou_thresh,\n",
    "            rpn_batch_size_per_image,\n",
    "            rpn_positive_fraction,\n",
    "            rpn_pre_nms_top_n,\n",
    "            rpn_post_nms_top_n,\n",
    "            rpn_nms_thresh,\n",
    "            score_thresh=rpn_score_thresh,\n",
    "        )\n",
    "        \n",
    "        roi_heads = OrientedRCNNRoIHeads(\n",
    "            # Box\n",
    "            box_roi_pool,\n",
    "            box_head,\n",
    "            box_predictor,\n",
    "            box_fg_iou_thresh,\n",
    "            box_bg_iou_thresh,\n",
    "            box_batch_size_per_image,\n",
    "            box_positive_fraction,\n",
    "            bbox_reg_weights,\n",
    "            box_score_thresh,\n",
    "            box_nms_thresh,\n",
    "            box_detections_per_img,\n",
    "            nms_thresh_rotated = 0.1\n",
    "        )\n",
    "                \n",
    "        super(OrientedRCNN, self).__init__(backbone, transform, rpn, roi_heads)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                During training, it returns a dict[Tensor] which contains the losses.\n",
    "                During testing, it returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            if targets is None:\n",
    "                torch._assert(False, \"targets should not be none when in training mode\")\n",
    "            else:\n",
    "                for target in targets:\n",
    "                    oboxes = target[\"oboxes\"]\n",
    "                    if isinstance(oboxes, torch.Tensor):\n",
    "                        torch._assert(\n",
    "                            len(oboxes.shape) == 2 and oboxes.shape[-1] == 5,\n",
    "                            f\"Expected target boxes to be a tensor of shape [N, 5], got {oboxes.shape}.\",\n",
    "                        )\n",
    "                    else:\n",
    "                        torch._assert(False, f\"Expected target boxes to be of type Tensor, got {type(oboxes)}.\")\n",
    "\n",
    "        return super(OrientedRCNN, self).forward(images, targets)\n",
    "\n",
    "\n",
    "pretrained: bool = True\n",
    "pretrained_backbone: bool = True\n",
    "progress: bool = True \n",
    "num_classes: Optional[int] = 91\n",
    "trainable_backbone_layers: Optional[int] = 5\n",
    "version: Literal[1, 2] = 1\n",
    "rpn_head: Optional[nn.Module] = None\n",
    "model: Optional[nn.Module] = None\n",
    "freeze_bn: bool = True\n",
    "\n",
    "if version == 1 and pretrained:\n",
    "    weights = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "    weights = FasterRCNN_ResNet50_FPN_Weights.verify(weights)\n",
    "    \n",
    "elif version == 2 and pretrained:\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.verify(weights)\n",
    "    \n",
    "else:\n",
    "    weights = None\n",
    "\n",
    "# TODO \n",
    "# if weights and num_classes is None:\n",
    "#     num_classes = len(weights.meta[\"categories\"])\n",
    "\n",
    "if not weights and pretrained_backbone:\n",
    "    weights_backbone = ResNet50_Weights.IMAGENET1K_V1\n",
    "\n",
    "else:\n",
    "    weights_backbone = None\n",
    "    \n",
    "is_faster_rcnn_trained = weights is not None\n",
    "is_backbone_trained = weights_backbone is not None or weights is not None\n",
    "\n",
    "if freeze_bn:\n",
    "    if not is_faster_rcnn_trained:\n",
    "        print(\"WARNING: pretrained weights are not used for training, freeze_bn is ignored.\")\n",
    "        fast_rcnn_norm_layer = nn.BatchNorm2d\n",
    "    else:\n",
    "        fast_rcnn_norm_layer = misc_nn_ops.FrozenBatchNorm2d\n",
    "\n",
    "    if not is_backbone_trained:\n",
    "        print(\"WARNING: pretrained backbone weights are not used for training, freeze_bn is ignored.\")\n",
    "        backbone_norm_layer = nn.BatchNorm2d\n",
    "    else:\n",
    "        backbone_norm_layer = misc_nn_ops.FrozenBatchNorm2d\n",
    "else:\n",
    "    fast_rcnn_norm_layer = nn.BatchNorm2d\n",
    "    backbone_norm_layer = nn.BatchNorm2d\n",
    "\n",
    "trainable_backbone_layers = _validate_trainable_layers(is_backbone_trained, trainable_backbone_layers, max_value=5, default_value=3)\n",
    "\n",
    "backbone = resnet50(weights=weights_backbone, progress=progress)\n",
    "backbone = _resnet_fpn_extractor(backbone, trainable_backbone_layers, [1, 2, 3, 4], norm_layer=backbone_norm_layer)\n",
    "\n",
    "rpn_anchor_generator = _default_anchor_generator()\n",
    "\n",
    "if version == 1:\n",
    "    rpn_head = rpn_head(backbone.out_channels, rpn_anchor_generator.num_anchors_per_location()[0], conv_depth=1)\n",
    "    box_head = None\n",
    "    \n",
    "elif version == 2:\n",
    "    rpn_head = rpn_head(backbone.out_channels, rpn_anchor_generator.num_anchors_per_location()[0], conv_depth=2)\n",
    "    box_head = FastRCNNConvFCHead(\n",
    "        (backbone.out_channels, 7, 7), [256, 256, 256, 256], [1024], norm_layer=fast_rcnn_norm_layer\n",
    "    )\n",
    "\n",
    "model = model(\n",
    "    backbone,\n",
    "    num_classes=num_classes,\n",
    "    rpn_anchor_generator=rpn_anchor_generator,\n",
    "    rpn_head=rpn_head,\n",
    "    box_head=box_head,\n",
    ")\n",
    "\n",
    "if weights:\n",
    "    model_state_dict = model.state_dict()\n",
    "    trained_state_dict = weights.get_state_dict(progress=progress)\n",
    "    for k, tensor in model_state_dict.items():\n",
    "        trained_tensor = trained_state_dict.get(k, None)\n",
    "        if trained_tensor is not None:\n",
    "            if tensor.shape == trained_tensor.shape:\n",
    "                model_state_dict[k] = trained_tensor\n",
    "            else:\n",
    "                print(f\"Skipped loading parameter {k} due to incompatible shapes: {tensor.shape} vs {trained_tensor.shape}\")\n",
    "        else:\n",
    "            print(f\"Skipped loading parameter {k} which is not in the model's state dict.\")\n",
    "\n",
    "    model.load_state_dict(model_state_dict, strict=False)\n",
    "    \n",
    "    if version == 1 and weights == FasterRCNN_ResNet50_FPN_Weights.COCO_V1:\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, misc_nn_ops.FrozenBatchNorm2d):\n",
    "                module.eps = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_v1 = FasterRCNN_ResNet50_FPN_Weights.COCO_V1\n",
    "weights_v2 = FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
    "weights_backbone = ResNet50_Weights.IMAGENET1K_V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['backbone.body.conv1.weight', 'backbone.body.bn1.weight', 'backbone.body.bn1.bias', 'backbone.body.bn1.running_mean', 'backbone.body.bn1.running_var', 'backbone.body.layer1.0.conv1.weight', 'backbone.body.layer1.0.bn1.weight', 'backbone.body.layer1.0.bn1.bias', 'backbone.body.layer1.0.bn1.running_mean', 'backbone.body.layer1.0.bn1.running_var', 'backbone.body.layer1.0.conv2.weight', 'backbone.body.layer1.0.bn2.weight', 'backbone.body.layer1.0.bn2.bias', 'backbone.body.layer1.0.bn2.running_mean', 'backbone.body.layer1.0.bn2.running_var', 'backbone.body.layer1.0.conv3.weight', 'backbone.body.layer1.0.bn3.weight', 'backbone.body.layer1.0.bn3.bias', 'backbone.body.layer1.0.bn3.running_mean', 'backbone.body.layer1.0.bn3.running_var', 'backbone.body.layer1.0.downsample.0.weight', 'backbone.body.layer1.0.downsample.1.weight', 'backbone.body.layer1.0.downsample.1.bias', 'backbone.body.layer1.0.downsample.1.running_mean', 'backbone.body.layer1.0.downsample.1.running_var', 'backbone.body.layer1.1.conv1.weight', 'backbone.body.layer1.1.bn1.weight', 'backbone.body.layer1.1.bn1.bias', 'backbone.body.layer1.1.bn1.running_mean', 'backbone.body.layer1.1.bn1.running_var', 'backbone.body.layer1.1.conv2.weight', 'backbone.body.layer1.1.bn2.weight', 'backbone.body.layer1.1.bn2.bias', 'backbone.body.layer1.1.bn2.running_mean', 'backbone.body.layer1.1.bn2.running_var', 'backbone.body.layer1.1.conv3.weight', 'backbone.body.layer1.1.bn3.weight', 'backbone.body.layer1.1.bn3.bias', 'backbone.body.layer1.1.bn3.running_mean', 'backbone.body.layer1.1.bn3.running_var', 'backbone.body.layer1.2.conv1.weight', 'backbone.body.layer1.2.bn1.weight', 'backbone.body.layer1.2.bn1.bias', 'backbone.body.layer1.2.bn1.running_mean', 'backbone.body.layer1.2.bn1.running_var', 'backbone.body.layer1.2.conv2.weight', 'backbone.body.layer1.2.bn2.weight', 'backbone.body.layer1.2.bn2.bias', 'backbone.body.layer1.2.bn2.running_mean', 'backbone.body.layer1.2.bn2.running_var', 'backbone.body.layer1.2.conv3.weight', 'backbone.body.layer1.2.bn3.weight', 'backbone.body.layer1.2.bn3.bias', 'backbone.body.layer1.2.bn3.running_mean', 'backbone.body.layer1.2.bn3.running_var', 'backbone.body.layer2.0.conv1.weight', 'backbone.body.layer2.0.bn1.weight', 'backbone.body.layer2.0.bn1.bias', 'backbone.body.layer2.0.bn1.running_mean', 'backbone.body.layer2.0.bn1.running_var', 'backbone.body.layer2.0.conv2.weight', 'backbone.body.layer2.0.bn2.weight', 'backbone.body.layer2.0.bn2.bias', 'backbone.body.layer2.0.bn2.running_mean', 'backbone.body.layer2.0.bn2.running_var', 'backbone.body.layer2.0.conv3.weight', 'backbone.body.layer2.0.bn3.weight', 'backbone.body.layer2.0.bn3.bias', 'backbone.body.layer2.0.bn3.running_mean', 'backbone.body.layer2.0.bn3.running_var', 'backbone.body.layer2.0.downsample.0.weight', 'backbone.body.layer2.0.downsample.1.weight', 'backbone.body.layer2.0.downsample.1.bias', 'backbone.body.layer2.0.downsample.1.running_mean', 'backbone.body.layer2.0.downsample.1.running_var', 'backbone.body.layer2.1.conv1.weight', 'backbone.body.layer2.1.bn1.weight', 'backbone.body.layer2.1.bn1.bias', 'backbone.body.layer2.1.bn1.running_mean', 'backbone.body.layer2.1.bn1.running_var', 'backbone.body.layer2.1.conv2.weight', 'backbone.body.layer2.1.bn2.weight', 'backbone.body.layer2.1.bn2.bias', 'backbone.body.layer2.1.bn2.running_mean', 'backbone.body.layer2.1.bn2.running_var', 'backbone.body.layer2.1.conv3.weight', 'backbone.body.layer2.1.bn3.weight', 'backbone.body.layer2.1.bn3.bias', 'backbone.body.layer2.1.bn3.running_mean', 'backbone.body.layer2.1.bn3.running_var', 'backbone.body.layer2.2.conv1.weight', 'backbone.body.layer2.2.bn1.weight', 'backbone.body.layer2.2.bn1.bias', 'backbone.body.layer2.2.bn1.running_mean', 'backbone.body.layer2.2.bn1.running_var', 'backbone.body.layer2.2.conv2.weight', 'backbone.body.layer2.2.bn2.weight', 'backbone.body.layer2.2.bn2.bias', 'backbone.body.layer2.2.bn2.running_mean', 'backbone.body.layer2.2.bn2.running_var', 'backbone.body.layer2.2.conv3.weight', 'backbone.body.layer2.2.bn3.weight', 'backbone.body.layer2.2.bn3.bias', 'backbone.body.layer2.2.bn3.running_mean', 'backbone.body.layer2.2.bn3.running_var', 'backbone.body.layer2.3.conv1.weight', 'backbone.body.layer2.3.bn1.weight', 'backbone.body.layer2.3.bn1.bias', 'backbone.body.layer2.3.bn1.running_mean', 'backbone.body.layer2.3.bn1.running_var', 'backbone.body.layer2.3.conv2.weight', 'backbone.body.layer2.3.bn2.weight', 'backbone.body.layer2.3.bn2.bias', 'backbone.body.layer2.3.bn2.running_mean', 'backbone.body.layer2.3.bn2.running_var', 'backbone.body.layer2.3.conv3.weight', 'backbone.body.layer2.3.bn3.weight', 'backbone.body.layer2.3.bn3.bias', 'backbone.body.layer2.3.bn3.running_mean', 'backbone.body.layer2.3.bn3.running_var', 'backbone.body.layer3.0.conv1.weight', 'backbone.body.layer3.0.bn1.weight', 'backbone.body.layer3.0.bn1.bias', 'backbone.body.layer3.0.bn1.running_mean', 'backbone.body.layer3.0.bn1.running_var', 'backbone.body.layer3.0.conv2.weight', 'backbone.body.layer3.0.bn2.weight', 'backbone.body.layer3.0.bn2.bias', 'backbone.body.layer3.0.bn2.running_mean', 'backbone.body.layer3.0.bn2.running_var', 'backbone.body.layer3.0.conv3.weight', 'backbone.body.layer3.0.bn3.weight', 'backbone.body.layer3.0.bn3.bias', 'backbone.body.layer3.0.bn3.running_mean', 'backbone.body.layer3.0.bn3.running_var', 'backbone.body.layer3.0.downsample.0.weight', 'backbone.body.layer3.0.downsample.1.weight', 'backbone.body.layer3.0.downsample.1.bias', 'backbone.body.layer3.0.downsample.1.running_mean', 'backbone.body.layer3.0.downsample.1.running_var', 'backbone.body.layer3.1.conv1.weight', 'backbone.body.layer3.1.bn1.weight', 'backbone.body.layer3.1.bn1.bias', 'backbone.body.layer3.1.bn1.running_mean', 'backbone.body.layer3.1.bn1.running_var', 'backbone.body.layer3.1.conv2.weight', 'backbone.body.layer3.1.bn2.weight', 'backbone.body.layer3.1.bn2.bias', 'backbone.body.layer3.1.bn2.running_mean', 'backbone.body.layer3.1.bn2.running_var', 'backbone.body.layer3.1.conv3.weight', 'backbone.body.layer3.1.bn3.weight', 'backbone.body.layer3.1.bn3.bias', 'backbone.body.layer3.1.bn3.running_mean', 'backbone.body.layer3.1.bn3.running_var', 'backbone.body.layer3.2.conv1.weight', 'backbone.body.layer3.2.bn1.weight', 'backbone.body.layer3.2.bn1.bias', 'backbone.body.layer3.2.bn1.running_mean', 'backbone.body.layer3.2.bn1.running_var', 'backbone.body.layer3.2.conv2.weight', 'backbone.body.layer3.2.bn2.weight', 'backbone.body.layer3.2.bn2.bias', 'backbone.body.layer3.2.bn2.running_mean', 'backbone.body.layer3.2.bn2.running_var', 'backbone.body.layer3.2.conv3.weight', 'backbone.body.layer3.2.bn3.weight', 'backbone.body.layer3.2.bn3.bias', 'backbone.body.layer3.2.bn3.running_mean', 'backbone.body.layer3.2.bn3.running_var', 'backbone.body.layer3.3.conv1.weight', 'backbone.body.layer3.3.bn1.weight', 'backbone.body.layer3.3.bn1.bias', 'backbone.body.layer3.3.bn1.running_mean', 'backbone.body.layer3.3.bn1.running_var', 'backbone.body.layer3.3.conv2.weight', 'backbone.body.layer3.3.bn2.weight', 'backbone.body.layer3.3.bn2.bias', 'backbone.body.layer3.3.bn2.running_mean', 'backbone.body.layer3.3.bn2.running_var', 'backbone.body.layer3.3.conv3.weight', 'backbone.body.layer3.3.bn3.weight', 'backbone.body.layer3.3.bn3.bias', 'backbone.body.layer3.3.bn3.running_mean', 'backbone.body.layer3.3.bn3.running_var', 'backbone.body.layer3.4.conv1.weight', 'backbone.body.layer3.4.bn1.weight', 'backbone.body.layer3.4.bn1.bias', 'backbone.body.layer3.4.bn1.running_mean', 'backbone.body.layer3.4.bn1.running_var', 'backbone.body.layer3.4.conv2.weight', 'backbone.body.layer3.4.bn2.weight', 'backbone.body.layer3.4.bn2.bias', 'backbone.body.layer3.4.bn2.running_mean', 'backbone.body.layer3.4.bn2.running_var', 'backbone.body.layer3.4.conv3.weight', 'backbone.body.layer3.4.bn3.weight', 'backbone.body.layer3.4.bn3.bias', 'backbone.body.layer3.4.bn3.running_mean', 'backbone.body.layer3.4.bn3.running_var', 'backbone.body.layer3.5.conv1.weight', 'backbone.body.layer3.5.bn1.weight', 'backbone.body.layer3.5.bn1.bias', 'backbone.body.layer3.5.bn1.running_mean', 'backbone.body.layer3.5.bn1.running_var', 'backbone.body.layer3.5.conv2.weight', 'backbone.body.layer3.5.bn2.weight', 'backbone.body.layer3.5.bn2.bias', 'backbone.body.layer3.5.bn2.running_mean', 'backbone.body.layer3.5.bn2.running_var', 'backbone.body.layer3.5.conv3.weight', 'backbone.body.layer3.5.bn3.weight', 'backbone.body.layer3.5.bn3.bias', 'backbone.body.layer3.5.bn3.running_mean', 'backbone.body.layer3.5.bn3.running_var', 'backbone.body.layer4.0.conv1.weight', 'backbone.body.layer4.0.bn1.weight', 'backbone.body.layer4.0.bn1.bias', 'backbone.body.layer4.0.bn1.running_mean', 'backbone.body.layer4.0.bn1.running_var', 'backbone.body.layer4.0.conv2.weight', 'backbone.body.layer4.0.bn2.weight', 'backbone.body.layer4.0.bn2.bias', 'backbone.body.layer4.0.bn2.running_mean', 'backbone.body.layer4.0.bn2.running_var', 'backbone.body.layer4.0.conv3.weight', 'backbone.body.layer4.0.bn3.weight', 'backbone.body.layer4.0.bn3.bias', 'backbone.body.layer4.0.bn3.running_mean', 'backbone.body.layer4.0.bn3.running_var', 'backbone.body.layer4.0.downsample.0.weight', 'backbone.body.layer4.0.downsample.1.weight', 'backbone.body.layer4.0.downsample.1.bias', 'backbone.body.layer4.0.downsample.1.running_mean', 'backbone.body.layer4.0.downsample.1.running_var', 'backbone.body.layer4.1.conv1.weight', 'backbone.body.layer4.1.bn1.weight', 'backbone.body.layer4.1.bn1.bias', 'backbone.body.layer4.1.bn1.running_mean', 'backbone.body.layer4.1.bn1.running_var', 'backbone.body.layer4.1.conv2.weight', 'backbone.body.layer4.1.bn2.weight', 'backbone.body.layer4.1.bn2.bias', 'backbone.body.layer4.1.bn2.running_mean', 'backbone.body.layer4.1.bn2.running_var', 'backbone.body.layer4.1.conv3.weight', 'backbone.body.layer4.1.bn3.weight', 'backbone.body.layer4.1.bn3.bias', 'backbone.body.layer4.1.bn3.running_mean', 'backbone.body.layer4.1.bn3.running_var', 'backbone.body.layer4.2.conv1.weight', 'backbone.body.layer4.2.bn1.weight', 'backbone.body.layer4.2.bn1.bias', 'backbone.body.layer4.2.bn1.running_mean', 'backbone.body.layer4.2.bn1.running_var', 'backbone.body.layer4.2.conv2.weight', 'backbone.body.layer4.2.bn2.weight', 'backbone.body.layer4.2.bn2.bias', 'backbone.body.layer4.2.bn2.running_mean', 'backbone.body.layer4.2.bn2.running_var', 'backbone.body.layer4.2.conv3.weight', 'backbone.body.layer4.2.bn3.weight', 'backbone.body.layer4.2.bn3.bias', 'backbone.body.layer4.2.bn3.running_mean', 'backbone.body.layer4.2.bn3.running_var', 'backbone.fpn.inner_blocks.0.weight', 'backbone.fpn.inner_blocks.0.bias', 'backbone.fpn.inner_blocks.1.weight', 'backbone.fpn.inner_blocks.1.bias', 'backbone.fpn.inner_blocks.2.weight', 'backbone.fpn.inner_blocks.2.bias', 'backbone.fpn.inner_blocks.3.weight', 'backbone.fpn.inner_blocks.3.bias', 'backbone.fpn.layer_blocks.0.weight', 'backbone.fpn.layer_blocks.0.bias', 'backbone.fpn.layer_blocks.1.weight', 'backbone.fpn.layer_blocks.1.bias', 'backbone.fpn.layer_blocks.2.weight', 'backbone.fpn.layer_blocks.2.bias', 'backbone.fpn.layer_blocks.3.weight', 'backbone.fpn.layer_blocks.3.bias', 'rpn.head.conv.weight', 'rpn.head.conv.bias', 'rpn.head.cls_logits.weight', 'rpn.head.cls_logits.bias', 'rpn.head.bbox_pred.weight', 'rpn.head.bbox_pred.bias', 'roi_heads.box_head.fc6.weight', 'roi_heads.box_head.fc6.bias', 'roi_heads.box_head.fc7.weight', 'roi_heads.box_head.fc7.bias', 'roi_heads.box_predictor.cls_score.weight', 'roi_heads.box_predictor.cls_score.bias', 'roi_heads.box_predictor.bbox_pred.weight', 'roi_heads.box_predictor.bbox_pred.bias'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_v1.get_state_dict(False).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
