{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Callable, Dict, List, Optional, Union\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.ops.feature_pyramid_network import ExtraFPNBlock, FeaturePyramidNetwork, LastLevelMaxPool\n",
    "\n",
    "from torchvision.models import mobilenet, resnet\n",
    "from torchvision.models._api import _get_enum_from_fn, WeightsEnum\n",
    "from torchvision.models._utils import handle_legacy_interface, IntermediateLayerGetter\n",
    "\n",
    "\n",
    "class BackboneWithFPN(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds a FPN on top of a model.\n",
    "    Internally, it uses torchvision.models._utils.IntermediateLayerGetter to\n",
    "    extract a submodel that returns the feature maps specified in return_layers.\n",
    "    The same limitations of IntermediateLayerGetter apply here.\n",
    "    Args:\n",
    "        backbone (nn.Module)\n",
    "        return_layers (Dict[name, new_name]): a dict containing the names\n",
    "            of the modules for which the activations will be returned as\n",
    "            the key of the dict, and the value of the dict is the name\n",
    "            of the returned activation (which the user can specify).\n",
    "        in_channels_list (List[int]): number of channels for each feature map\n",
    "            that is returned, in the order they are present in the OrderedDict\n",
    "        out_channels (int): number of channels in the FPN.\n",
    "        norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None\n",
    "    Attributes:\n",
    "        out_channels (int): the number of channels in the FPN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        return_layers: Dict[str, str],\n",
    "        in_channels_list: List[int],\n",
    "        out_channels: int,\n",
    "        extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if extra_blocks is None:\n",
    "            extra_blocks = LastLevelMaxPool()\n",
    "\n",
    "        self.body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=in_channels_list,\n",
    "            out_channels=out_channels,\n",
    "            extra_blocks=extra_blocks,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x: Tensor) -> Dict[str, Tensor]:\n",
    "        x = self.body(x)\n",
    "        x = self.fpn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@handle_legacy_interface(\n",
    "    weights=(\n",
    "        \"pretrained\",\n",
    "        lambda kwargs: _get_enum_from_fn(resnet.__dict__[kwargs[\"backbone_name\"]])[\"IMAGENET1K_V1\"],\n",
    "    ),\n",
    ")\n",
    "def resnet_fpn_backbone(\n",
    "    *,\n",
    "    backbone_name: str,\n",
    "    weights: Optional[WeightsEnum],\n",
    "    norm_layer: Callable[..., nn.Module] = misc_nn_ops.FrozenBatchNorm2d,\n",
    "    trainable_layers: int = 3,\n",
    "    returned_layers: Optional[List[int]] = None,\n",
    "    extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    ") -> BackboneWithFPN:\n",
    "    \"\"\"\n",
    "    Constructs a specified ResNet backbone with FPN on top. Freezes the specified number of layers in the backbone.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "        >>> backbone = resnet_fpn_backbone('resnet50', weights=ResNet50_Weights.DEFAULT, trainable_layers=3)\n",
    "        >>> # get some dummy image\n",
    "        >>> x = torch.rand(1,3,64,64)\n",
    "        >>> # compute the output\n",
    "        >>> output = backbone(x)\n",
    "        >>> print([(k, v.shape) for k, v in output.items()])\n",
    "        >>> # returns\n",
    "        >>>   [('0', torch.Size([1, 256, 16, 16])),\n",
    "        >>>    ('1', torch.Size([1, 256, 8, 8])),\n",
    "        >>>    ('2', torch.Size([1, 256, 4, 4])),\n",
    "        >>>    ('3', torch.Size([1, 256, 2, 2])),\n",
    "        >>>    ('pool', torch.Size([1, 256, 1, 1]))]\n",
    "\n",
    "    Args:\n",
    "        backbone_name (string): resnet architecture. Possible values are 'resnet18', 'resnet34', 'resnet50',\n",
    "             'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2'\n",
    "        weights (WeightsEnum, optional): The pretrained weights for the model\n",
    "        norm_layer (callable): it is recommended to use the default value. For details visit:\n",
    "            (https://github.com/facebookresearch/maskrcnn-benchmark/issues/267)\n",
    "        trainable_layers (int): number of trainable (not frozen) layers starting from final block.\n",
    "            Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable.\n",
    "        returned_layers (list of int): The layers of the network to return. Each entry must be in ``[1, 4]``.\n",
    "            By default, all layers are returned.\n",
    "        extra_blocks (ExtraFPNBlock or None): if provided, extra operations will\n",
    "            be performed. It is expected to take the fpn features, the original\n",
    "            features and the names of the original features as input, and returns\n",
    "            a new list of feature maps and their corresponding names. By\n",
    "            default, a ``LastLevelMaxPool`` is used.\n",
    "    \"\"\"\n",
    "    backbone = resnet.__dict__[backbone_name](weights=weights, norm_layer=norm_layer)\n",
    "    return _resnet_fpn_extractor(backbone, trainable_layers, returned_layers, extra_blocks)\n",
    "\n",
    "\n",
    "def _resnet_fpn_extractor(\n",
    "    backbone: resnet.ResNet,\n",
    "    trainable_layers: int,\n",
    "    returned_layers: Optional[List[int]] = None,\n",
    "    extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    "    norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    ") -> BackboneWithFPN:\n",
    "\n",
    "    # select layers that won't be frozen\n",
    "    if trainable_layers < 0 or trainable_layers > 5:\n",
    "        raise ValueError(f\"Trainable layers should be in the range [0,5], got {trainable_layers}\")\n",
    "    layers_to_train = [\"layer4\", \"layer3\", \"layer2\", \"layer1\", \"conv1\"][:trainable_layers]\n",
    "    if trainable_layers == 5:\n",
    "        layers_to_train.append(\"bn1\")\n",
    "    for name, parameter in backbone.named_parameters():\n",
    "        if all([not name.startswith(layer) for layer in layers_to_train]):\n",
    "            parameter.requires_grad_(False)\n",
    "\n",
    "    if extra_blocks is None:\n",
    "        extra_blocks = LastLevelMaxPool()\n",
    "\n",
    "    if returned_layers is None:\n",
    "        returned_layers = [1, 2, 3, 4]\n",
    "    if min(returned_layers) <= 0 or max(returned_layers) >= 5:\n",
    "        raise ValueError(f\"Each returned layer should be in the range [1,4]. Got {returned_layers}\")\n",
    "    return_layers = {f\"layer{k}\": str(v) for v, k in enumerate(returned_layers)}\n",
    "\n",
    "    in_channels_stage2 = backbone.inplanes // 8\n",
    "    in_channels_list = [in_channels_stage2 * 2 ** (i - 1) for i in returned_layers]\n",
    "    out_channels = 256\n",
    "    return BackboneWithFPN(\n",
    "        backbone, return_layers, in_channels_list, out_channels, extra_blocks=extra_blocks, norm_layer=norm_layer\n",
    "    )\n",
    "\n",
    "\n",
    "def _validate_trainable_layers(\n",
    "    is_trained: bool,\n",
    "    trainable_backbone_layers: Optional[int],\n",
    "    max_value: int,\n",
    "    default_value: int,\n",
    ") -> int:\n",
    "    # don't freeze any layers if pretrained model or backbone is not used\n",
    "    if not is_trained:\n",
    "        if trainable_backbone_layers is not None:\n",
    "            warnings.warn(\n",
    "                \"Changing trainable_backbone_layers has not effect if \"\n",
    "                \"neither pretrained nor pretrained_backbone have been set to True, \"\n",
    "                f\"falling back to trainable_backbone_layers={max_value} so that all layers are trainable\"\n",
    "            )\n",
    "        trainable_backbone_layers = max_value\n",
    "\n",
    "    # by default freeze first blocks\n",
    "    if trainable_backbone_layers is None:\n",
    "        trainable_backbone_layers = default_value\n",
    "    if trainable_backbone_layers < 0 or trainable_backbone_layers > max_value:\n",
    "        raise ValueError(\n",
    "            f\"Trainable backbone layers should be in the range [0,{max_value}], got {trainable_backbone_layers} \"\n",
    "        )\n",
    "    return trainable_backbone_layers\n",
    "\n",
    "\n",
    "@handle_legacy_interface(\n",
    "    weights=(\n",
    "        \"pretrained\",\n",
    "        lambda kwargs: _get_enum_from_fn(mobilenet.__dict__[kwargs[\"backbone_name\"]])[\"IMAGENET1K_V1\"],\n",
    "    ),\n",
    ")\n",
    "def mobilenet_backbone(\n",
    "    *,\n",
    "    backbone_name: str,\n",
    "    weights: Optional[WeightsEnum],\n",
    "    fpn: bool,\n",
    "    norm_layer: Callable[..., nn.Module] = misc_nn_ops.FrozenBatchNorm2d,\n",
    "    trainable_layers: int = 2,\n",
    "    returned_layers: Optional[List[int]] = None,\n",
    "    extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    ") -> nn.Module:\n",
    "    backbone = mobilenet.__dict__[backbone_name](weights=weights, norm_layer=norm_layer)\n",
    "    return _mobilenet_extractor(backbone, fpn, trainable_layers, returned_layers, extra_blocks)\n",
    "\n",
    "\n",
    "def _mobilenet_extractor(\n",
    "    backbone: Union[mobilenet.MobileNetV2, mobilenet.MobileNetV3],\n",
    "    fpn: bool,\n",
    "    trainable_layers: int,\n",
    "    returned_layers: Optional[List[int]] = None,\n",
    "    extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    "    norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    ") -> nn.Module:\n",
    "    backbone = backbone.features\n",
    "    # Gather the indices of blocks which are strided. These are the locations of C1, ..., Cn-1 blocks.\n",
    "    # The first and last blocks are always included because they are the C0 (conv1) and Cn.\n",
    "    stage_indices = [0] + [i for i, b in enumerate(backbone) if getattr(b, \"_is_cn\", False)] + [len(backbone) - 1]\n",
    "    num_stages = len(stage_indices)\n",
    "\n",
    "    # find the index of the layer from which we won't freeze\n",
    "    if trainable_layers < 0 or trainable_layers > num_stages:\n",
    "        raise ValueError(f\"Trainable layers should be in the range [0,{num_stages}], got {trainable_layers} \")\n",
    "    freeze_before = len(backbone) if trainable_layers == 0 else stage_indices[num_stages - trainable_layers]\n",
    "\n",
    "    for b in backbone[:freeze_before]:\n",
    "        for parameter in b.parameters():\n",
    "            parameter.requires_grad_(False)\n",
    "\n",
    "    out_channels = 256\n",
    "    if fpn:\n",
    "        if extra_blocks is None:\n",
    "            extra_blocks = LastLevelMaxPool()\n",
    "\n",
    "        if returned_layers is None:\n",
    "            returned_layers = [num_stages - 2, num_stages - 1]\n",
    "        if min(returned_layers) < 0 or max(returned_layers) >= num_stages:\n",
    "            raise ValueError(f\"Each returned layer should be in the range [0,{num_stages - 1}], got {returned_layers} \")\n",
    "        return_layers = {f\"{stage_indices[k]}\": str(v) for v, k in enumerate(returned_layers)}\n",
    "\n",
    "        in_channels_list = [backbone[stage_indices[i]].out_channels for i in returned_layers]\n",
    "        return BackboneWithFPN(\n",
    "            backbone, return_layers, in_channels_list, out_channels, extra_blocks=extra_blocks, norm_layer=norm_layer\n",
    "        )\n",
    "    else:\n",
    "        m = nn.Sequential(\n",
    "            backbone,\n",
    "            # depthwise linear combination of channels to reduce their size\n",
    "            nn.Conv2d(backbone[-1].out_channels, out_channels, 1),\n",
    "        )\n",
    "        m.out_channels = out_channels  # type: ignore[assignment]\n",
    "        return m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = mobilenet_v3_large(weights=weights_backbone, progress=progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v3_large\n",
    "weights_backbone = None\n",
    "progress = None\n",
    "backbone = mobilenet_v3_large(weights=weights_backbone, progress=progress)\n",
    "backbone = _mobilenet_extractor(backbone, fpn=True, trainable_layers=6, returned_layers=[1, 2, 3, 4])\t\n",
    "mb3_out = backbone(torch.randn(4, 3, 224, 224))\n",
    "\n",
    "from torchvision.models import resnet50\n",
    "backbone = resnet50(weights=weights_backbone, progress=progress)\n",
    "backbone = _resnet_fpn_extractor(backbone, trainable_layers=5, returned_layers=[1, 2, 3, 4])\t\n",
    "r50_out = backbone(torch.randn(4, 3, 224, 224))\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "backbone = resnet18(weights=weights_backbone, progress=progress)\n",
    "backbone = _resnet_fpn_extractor(backbone, trainable_layers=5, returned_layers=[1, 2, 3, 4])\t\n",
    "r18_out = backbone(torch.randn(4, 3, 224, 224))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 256, 56, 56])\n",
      "1 torch.Size([4, 256, 28, 28])\n",
      "2 torch.Size([4, 256, 14, 14])\n",
      "3 torch.Size([4, 256, 7, 7])\n",
      "pool torch.Size([4, 256, 4, 4])\n",
      "0 torch.Size([4, 256, 56, 56])\n",
      "1 torch.Size([4, 256, 28, 28])\n",
      "2 torch.Size([4, 256, 14, 14])\n",
      "3 torch.Size([4, 256, 7, 7])\n",
      "pool torch.Size([4, 256, 4, 4])\n",
      "0 torch.Size([4, 256, 56, 56])\n",
      "1 torch.Size([4, 256, 28, 28])\n",
      "2 torch.Size([4, 256, 14, 14])\n",
      "3 torch.Size([4, 256, 7, 7])\n",
      "pool torch.Size([4, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for k, v in mb3_out.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "for k, v in r50_out.items():\n",
    "    print(k, v.shape)\n",
    "    \n",
    "for k, v in r18_out.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import efficientnet_b0, efficientnet_b4, efficientnet_b7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = efficientnet_b7(weights=None, progress=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 64, 112, 112]) False\n",
      "1 torch.Size([4, 32, 112, 112]) False\n",
      "2 torch.Size([4, 48, 56, 56]) False\n",
      "3 torch.Size([4, 80, 28, 28]) False\n",
      "4 torch.Size([4, 160, 14, 14]) False\n",
      "5 torch.Size([4, 224, 14, 14]) False\n",
      "6 torch.Size([4, 384, 7, 7]) False\n",
      "7 torch.Size([4, 640, 7, 7]) False\n",
      "8 torch.Size([4, 2560, 7, 7]) False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 3, 224, 224)\n",
    "for i, layer in enumerate(backbone.features):\n",
    "    x = layer(x)\n",
    "    print(i, x.shape, getattr(layer, \"_is_cn\", False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import EfficientNet\n",
    "from torchvision.ops.misc import Conv2dNormActivation\n",
    "\n",
    "def _efficientnet_extractor(\n",
    "    backbone: EfficientNet,\n",
    "    trainable_layers: int,\n",
    "    returned_layers: Optional[List[int]] = None,\n",
    "    extra_blocks: Optional[ExtraFPNBlock] = None,\n",
    "    norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    ") -> nn.Module:\n",
    "    backbone = backbone.features\n",
    "    # Gather the indices of blocks which are strided. These are the locations of C1, ..., Cn-1 blocks.\n",
    "    # The first and last blocks are always included because they are the C0 (conv1) and Cn.\n",
    "    stage_indices = [0, 2, 3, 4, 6]\n",
    "    num_stages = len(stage_indices)\n",
    "\n",
    "    # find the index of the layer from which we won't freeze\n",
    "    if trainable_layers < 0 or trainable_layers > num_stages:\n",
    "        raise ValueError(f\"Trainable layers should be in the range [0,{num_stages}], got {trainable_layers} \")\n",
    "    freeze_before = len(backbone) if trainable_layers == 0 else stage_indices[num_stages - trainable_layers]\n",
    "\n",
    "    for b in backbone[:freeze_before]:\n",
    "        for parameter in b.parameters():\n",
    "            parameter.requires_grad_(False)\n",
    "\n",
    "    out_channels = 256\n",
    "    if extra_blocks is None:\n",
    "        extra_blocks = LastLevelMaxPool()\n",
    "\n",
    "    if returned_layers is None:\n",
    "        returned_layers = [num_stages - 2, num_stages - 1]\n",
    "    if min(returned_layers) < 0 or max(returned_layers) >= num_stages:\n",
    "        raise ValueError(f\"Each returned layer should be in the range [0,{num_stages - 1}], got {returned_layers} \")\n",
    "    return_layers = {f\"{stage_indices[k]}\": str(v) for v, k in enumerate(returned_layers)}\n",
    "    print(return_layers)\n",
    "    \n",
    "    in_channels_list = []\n",
    "    for i in returned_layers:\n",
    "        layer = backbone[stage_indices[i]]\n",
    "        if isinstance(layer, Conv2dNormActivation):\n",
    "            in_channels_list.append(layer.out_channels)\n",
    "\n",
    "        else:\n",
    "            in_channels_list.append(layer[-1].out_channels)\n",
    "            \n",
    "    return BackboneWithFPN(\n",
    "        backbone, return_layers, in_channels_list, out_channels, extra_blocks=extra_blocks, norm_layer=norm_layer\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2': '0', '3': '1', '4': '2', '6': '3'}\n"
     ]
    }
   ],
   "source": [
    "backbone = _efficientnet_extractor(backbone, 5, [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_out = backbone(torch.randn(4, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 256, 56, 56])\n",
      "1 torch.Size([4, 256, 28, 28])\n",
      "2 torch.Size([4, 256, 14, 14])\n",
      "3 torch.Size([4, 256, 7, 7])\n",
      "pool torch.Size([4, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "for k, v in eff_out.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
