{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Det_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "from xml.etree import ElementTree as ET\n",
    "from statistics import mean\n",
    "\n",
    "# Function to rotate a point anticlockwise around an origin\n",
    "def rotate_point(x, y, angle_degrees, origin):\n",
    "    angle_radians = math.radians(-angle_degrees)\n",
    "    ox, oy = origin\n",
    "    qx = ox + math.cos(angle_radians) * (x - ox) + math.sin(angle_radians) * (y - oy)\n",
    "    qy = oy - math.sin(angle_radians) * (x - ox) + math.cos(angle_radians) * (y - oy)\n",
    "    return qx, qy\n",
    "\n",
    "def parse_labelme_anns(base_path):\n",
    "    xml_files_list = glob.glob(base_path + \"/*.xml\")\n",
    "    # Initialize an empty dictionary to store all parsed details from multiple files\n",
    "    all_files_details = {}\n",
    "\n",
    "    # Loop through each XML data string in the list\n",
    "    for xml_path in xml_files_list:\n",
    "        with open(xml_path, 'r') as f:\n",
    "            xml_data = f.read()\n",
    "        \n",
    "        # Parse XML data\n",
    "        root = ET.fromstring(xml_data)\n",
    "        \n",
    "        # Get the filename\n",
    "        filename = root.find('filename').text if root.find('filename') is not None else 'Unknown'\n",
    "            \n",
    "        # Initialize an empty list to store object details for this file\n",
    "        file_object_details = []\n",
    "        \n",
    "        # Loop through each object in the XML\n",
    "        for obj in root.findall('object'):\n",
    "            # Check if the object has a name and a polygon\n",
    "            if obj.find('name') is not None and obj.find('polygon') is not None:\n",
    "                name = obj.find('name').text  # Get the class label/name\n",
    "                polygon = obj.find('polygon')  # Get the polygon element\n",
    "                coordinates = []  # Initialize an empty list to store coordinates\n",
    "                \n",
    "                # Get the rotation angle from attributes (default to 0 if not found)\n",
    "                rotation_angle = float(obj.find('attributes').text.split('=')[-1]) if obj.find('attributes') is not None else 0.0\n",
    "                \n",
    "                # Loop through each point in the polygon\n",
    "                for pt in polygon.findall('pt'):\n",
    "                    x = float(pt.find('x').text)  # Get the x-coordinate and convert to float\n",
    "                    y = float(pt.find('y').text)  # Get the y-coordinate and convert to float\n",
    "                    coordinates.append((x, y))  # Append the (x, y) tuple to the coordinates list\n",
    "\n",
    "                # Calculate the center of the polygon\n",
    "                center_x = mean(x for x, y in coordinates)\n",
    "                center_y = mean(y for x, y in coordinates)\n",
    "                    \n",
    "                # Rotate the coordinates around the center\n",
    "                rotated_coordinates = [rotate_point(x, y, rotation_angle, (center_x, center_y)) for x, y in coordinates]\n",
    "\n",
    "                file_object_details.append({\n",
    "                    'name': name,\n",
    "                    'coordinates': rotated_coordinates,\n",
    "                    'rotation': rotation_angle\n",
    "                })\n",
    "        \n",
    "        # Add this file's object details to the all_files_details dictionary\n",
    "        all_files_details[filename] = file_object_details\n",
    "    return all_files_details\n",
    "all_files_details = parse_labelme_anns(\"/datasets/labelme_detdemo/default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_path = \"/datasets/detdemo/annfiles\"\n",
    "imgs_path = \"/datasets/detdemo/images\"\n",
    "trainval_path = \"/datasets/detdemo/trainval\"\n",
    "test_path = \"/datasets/detdemo/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "trainval_imgs_path = os.path.join(trainval_path, \"images\")\n",
    "trainval_anns_path = os.path.join(trainval_path, \"annfiles\")\n",
    "test_imgs_path = os.path.join(test_path, \"images\")\n",
    "test_anns_path = os.path.join(test_path, \"annfiles\")\n",
    "\n",
    "os.makedirs(trainval_imgs_path, exist_ok=True)\n",
    "os.makedirs(trainval_anns_path, exist_ok=True)\n",
    "os.makedirs(test_imgs_path, exist_ok=True)\n",
    "os.makedirs(test_anns_path, exist_ok=True)\n",
    "os.makedirs(anns_path, exist_ok=True)\n",
    "for filename, objects in all_files_details.items():\n",
    "    if not objects:\n",
    "        continue\n",
    "    \n",
    "    ann_file = os.path.join(anns_path, filename.replace('.jpg', '.txt'))\n",
    "    lines = []\n",
    "    \n",
    "    for object in objects:\n",
    "        line = []\n",
    "        for coor in object['coordinates']:\n",
    "            for item in coor:\n",
    "                line.append(str(item))\n",
    "        line.append(object['name'])\n",
    "        line += \"0\\n\"\n",
    "        lines.append(\" \".join(line))\n",
    "\n",
    "    with open(ann_file, \"w\") as f:\n",
    "        f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import shutil\n",
    "txtfiles = glob.glob(anns_path + \"/*.txt\")\n",
    "imgfiles = [txtfile.replace(\"annfiles\", \"images\").replace(\".txt\", \".jpg\") for txtfile in txtfiles]\n",
    "\n",
    "n = len(txtfiles)\n",
    "n_train = int(n*0.9)\n",
    "\n",
    "train_txtfiles = txtfiles[:n_train]\n",
    "train_imgfiles = imgfiles[:n_train]\n",
    "\n",
    "for train_txtfile, train_imgfile in zip(train_txtfiles, train_imgfiles):\n",
    "    txtfilename = train_txtfile.split(\"/\")[-1]\n",
    "    imgfilename = train_imgfile.split(\"/\")[-1]\n",
    "    shutil.copy(train_txtfile, os.path.join(trainval_anns_path, txtfilename))\n",
    "    shutil.copy(train_imgfile, os.path.join(trainval_imgs_path, imgfilename))\n",
    "    \n",
    "test_txtfiles = txtfiles[n_train:]\n",
    "test_imgfiles = imgfiles[n_train:]\n",
    "\n",
    "for test_txtfile, test_imgfile in zip(test_txtfiles, test_imgfiles):\n",
    "    txtfilename = test_txtfile.split(\"/\")[-1]\n",
    "    imgfilename = test_imgfile.split(\"/\")[-1]\n",
    "    shutil.copy(test_txtfile, os.path.join(test_anns_path, txtfilename))\n",
    "    shutil.copy(test_imgfile, os.path.join(test_imgs_path, imgfilename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagesize import get\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jmc = \"/datasets/JMC\"\n",
    "images = glob.glob(jmc + \"/*.png\")\n",
    "images_by_size = {}\n",
    "sizes = set()\n",
    "for image in images:\n",
    "    images_by_size.setdefault(get(image), []).append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 800) 2522\n",
      "(2520, 800) 230\n"
     ]
    }
   ],
   "source": [
    "for _, images in images_by_size.items():\n",
    "    print(_, len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "new_dir  = \"/datasets/JMC_merged\"\n",
    "os.makedirs(new_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_images = list(filter(lambda x: \"-8\" in x, images))\n",
    "one_images = list(filter(lambda x: \"-1\" in x, images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/841 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 840/841 [17:20<00:01,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(0, len(eight_images), 3)):\n",
    "    try:\n",
    "        a = eight_images[i]\n",
    "        b = eight_images[i+1]\n",
    "        c = eight_images[i+2]\n",
    "    except:\n",
    "        break\n",
    "    # Open the images\n",
    "    image1 = cv2.imread(a)\n",
    "    image2 = cv2.imread(b)\n",
    "    image3 = cv2.imread(c)\n",
    "\n",
    "    # Make sure they have the same width\n",
    "    if image1.shape[1] != image2.shape[1] != image3.shape[1]:\n",
    "        print(\"Images do not have the same width\")\n",
    "    else:\n",
    "        # Get dimensions\n",
    "        height1, width = image1.shape[:2]\n",
    "        height2, _ = image2.shape[:2]\n",
    "        height3, _ = image3.shape[:2]\n",
    "\n",
    "        # Create a new array with the combined height\n",
    "        new_image = np.zeros((height1 + height2 + height3, width, 3), dtype=np.uint8)\n",
    "\n",
    "        # Copy the original images into this new array\n",
    "        new_image[0:height1, 0:width] = image1\n",
    "        new_image[height1:height1 + height2, 0:width] = image2\n",
    "        new_image[height1 + height2:height1 + height2 + height3, 0:width] = image3\n",
    "\n",
    "        new_image_name = a.split(\"/\")[-1][:-4] + \"_\" + b.split(\"/\")[-1][:-4] + \"_\" + c.split(\"/\")[-1][:-4] + \".png\"\n",
    "        new_image_path = os.path.join(new_dir, new_image_name)\n",
    "        # Save the new image\n",
    "        cv2.imwrite(new_image_path, new_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 76/77 [02:01<00:01,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(0, len(one_images), 3)):\n",
    "    try:\n",
    "        a = one_images[i]\n",
    "        b = one_images[i+1]\n",
    "        c = one_images[i+2]\n",
    "    \n",
    "    except:\n",
    "        break\n",
    "    # Open the images\n",
    "    image1 = cv2.imread(a)\n",
    "    image2 = cv2.imread(b)\n",
    "    image3 = cv2.imread(c)\n",
    "\n",
    "    # Make sure they have the same width\n",
    "    if image1.shape[1] != image2.shape[1] != image3.shape[1]:\n",
    "        print(\"Images do not have the same width\")\n",
    "    else:\n",
    "        # Get dimensions\n",
    "        height1, width = image1.shape[:2]\n",
    "        height2, _ = image2.shape[:2]\n",
    "        height3, _ = image3.shape[:2]\n",
    "\n",
    "        # Create a new array with the combined height\n",
    "        new_image = np.zeros((height1 + height2 + height3, width, 3), dtype=np.uint8)\n",
    "\n",
    "        # Copy the original images into this new array\n",
    "        new_image[0:height1, 0:width] = image1\n",
    "        new_image[height1:height1 + height2, 0:width] = image2\n",
    "        new_image[height1 + height2:height1 + height2 + height3, 0:width] = image3\n",
    "\n",
    "        new_image_name = a.split(\"/\")[-1][:-4] + \"_\" + b.split(\"/\")[-1][:-4] + \"_\" + c.split(\"/\")[-1][:-4] + \".png\"\n",
    "        new_image_path = os.path.join(new_dir, new_image_name)\n",
    "        # Save the new image\n",
    "        cv2.imwrite(new_image_path, new_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=torch.load(\"/odet_copy/datasets/detdemo.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_path =\"/datasets/detdemo/\"\n",
    "new_path = \"/app/input/dataset/detdemo/\"\n",
    "for split in (\"train\", \"test\"):\n",
    "    t = a[split]\n",
    "    for i in t:\n",
    "        i[\"image_path\"] = i[\"image_path\"].replace(orig_path, new_path)\n",
    "        i[\"ann_path\"] = i[\"ann_path\"].replace(orig_path, new_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(a, \"/odet_copy/datasets/detdemo.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 916/916 [09:19<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# pip install opencv-python tqdm numpy \n",
    "import argparse \n",
    "\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--path\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    means = np.zeros(3)\n",
    "    stds = np.zeros(3)\n",
    "    paths = os.listdir(args.path)\n",
    "    for img_path in tqdm(paths, \"Calculating stats\"):\n",
    "        img = cv2.imread(os.path.join(args.path, img_path))\n",
    "        means += np.mean(img, axis=(0, 1)) # across height and width\n",
    "        stds += np.std(img, axis=(0, 1))\n",
    "\n",
    "    means /= len(paths)\n",
    "    stds /= len(paths)\n",
    "\n",
    "    # BGR to RGB\n",
    "    means = means[::-1]\n",
    "    stds = stds[::-1]\n",
    "    \n",
    "    print(\"RGB stats\")\n",
    "    print(\"Means:\", means)\n",
    "    print(\"Stds:\", stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66.03297039, 66.81605603, 64.15269246])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47.82983312, 45.1165196 , 44.93892323])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
